{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oscarB1nar10/NLP/blob/main/gpt_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeZVlKekz67p"
      },
      "source": [
        "#### Mini gpt (character-level) implemented from scratch in Pythorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b88ystlSzwEh"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import argparse\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J0lfwTP0kNr"
      },
      "outputs": [],
      "source": [
        "# --------\n",
        "# Config dataclass\n",
        "# --------\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "  vocab_size: int\n",
        "  n_embd: int = 256 # Model width\n",
        "  n_head: int = 8 # Number of attention heads\n",
        "  n_layer: int = 6 # Number of transformer blocks\n",
        "  sequence_length: int = 256 # Max context lenght (sequence lenght)\n",
        "  dropout: float = 0.1\n",
        "\n",
        "\n",
        "# -------------\n",
        "# Tokenizer (char-level)\n",
        "# ---------------\n",
        "class CharTokenizer:\n",
        "  def __init__(self, text: str):\n",
        "    chars = sorted(list(set(text)))\n",
        "    self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "    self.itos = {i: ch for i, ch in enumerate(chars)}\n",
        "    self.vocab_size = len(chars)\n",
        "\n",
        "  def encode(self, s: str):\n",
        "    return [self.stoi[char] for char in s]\n",
        "\n",
        "  def decode(self, ids):\n",
        "    return ''.join(str(self.itos[i]) for i in ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8AWStTc2eOt",
        "outputId": "fd45ab71-fa1d-4629-ba65-efcd6618d857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 2, 2, 3]\n",
            "<class 'list'>\n",
            "<class 'str'>\n",
            "hello\n"
          ]
        }
      ],
      "source": [
        "# Example of use\n",
        "ct = CharTokenizer(\"hello\")\n",
        "encoding = ct.encode(\"hello\")\n",
        "print(encoding)\n",
        "print(type(encoding))\n",
        "decoding = ct.decode(encoding)\n",
        "print(type(decoding))\n",
        "print(decoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6cV7EH0LcdE"
      },
      "outputs": [],
      "source": [
        "# -------------\n",
        "# Dataset utilities\n",
        "# -------------\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, data: torch.Tensor, sequence_length: int):\n",
        "    self.data = data\n",
        "    self.sequence_length = sequence_length\n",
        "\n",
        "  # The last usable starting index for a full-length chunck\n",
        "  # Example:\n",
        "  # data = [0, 1, 2, 3, 4, 5, 6]\n",
        "  # sequence_length = 4\n",
        "  # len(data) - sequence_length = 7 - 4 = 3\n",
        "  # The dataset yields:\n",
        "  # idx = 0 → chunk [0, 1, 2, 3, 4]\n",
        "  # idx = 1 → chunk [1, 2, 3, 4, 5]\n",
        "  # idx = 2 → chunk [2, 3, 4, 5, 6]\n",
        "  # If we allowed idx = 3, it would try to slice [3:8] — but there’s no element 7, so it would break.\n",
        "  def __len__(self):\n",
        "    return len(self.data) - self.sequence_length\n",
        "\n",
        "  # Get the input and the target\n",
        "  # Example:\n",
        "  # chunk = [a, b, c, d, e]\n",
        "  # x = [a, b, c, d]\n",
        "  # y = [b, c, d, e]\n",
        "  def __getitem__(self, idx):\n",
        "    chunk = self.data[idx: idx + self.sequence_length + 1]\n",
        "    x = chunk[:-1]\n",
        "    y = chunk[1:]\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7azm9VcUA81",
        "outputId": "bb884491-a2c8-440e-db10-c8a16c854785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset length:  16\n",
            "Sample 0:\n",
            "x: tensor([0, 1, 2, 3])\n",
            "y: tensor([1, 2, 3, 4])\n",
            "Sample 1:\n",
            "x: tensor([1, 2, 3, 4])\n",
            "y: tensor([2, 3, 4, 5])\n",
            "Sample 2:\n",
            "x: tensor([2, 3, 4, 5])\n",
            "y: tensor([3, 4, 5, 6])\n",
            "Sample 3:\n",
            "x: tensor([3, 4, 5, 6])\n",
            "y: tensor([4, 5, 6, 7])\n"
          ]
        }
      ],
      "source": [
        "# Example of use\n",
        "# Integers representing token IDs\n",
        "data = torch.arange(20)\n",
        "sequence_length = 4\n",
        "\n",
        "dataset = TextDataset(data, sequence_length)\n",
        "\n",
        "# Check length\n",
        "print(\"Dataset length: \", len(dataset)) # len(data) - sequence_length\n",
        "\n",
        "# Inspect some samples\n",
        "for i in range(4):\n",
        "  x, y = dataset[i]\n",
        "  print(f\"Sample {i}:\")\n",
        "  print(\"x:\", x)\n",
        "  print(\"y:\", y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPxnE3Z0zfed"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# Model components\n",
        "# -------------\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, n_embd, eps=1e-5):\n",
        "    super().__init__()\n",
        "    # Learnable parameter, this parameter scales the normalized output\n",
        "    self.gamma = nn.Parameter(torch.ones(n_embd))\n",
        "    # Learnable parameter, this parameter shift the distribution back to any mean\n",
        "    # the model needs.\n",
        "    self.beta = nn.Parameter(torch.zeros(n_embd))\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, unbiased=False, keepdim=True)\n",
        "    x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.gamma * x_hat + self.beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2dBczpc5V4G",
        "outputId": "9a97af5d-1e14-4b8d-a3a3-2be2d854b877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tensor([[[ 1.,  2.,  3.,  4.],\n",
            "         [ 2.,  2.,  2.,  2.],\n",
            "         [ 0., -1.,  1.,  2.]],\n",
            "\n",
            "        [[ 3.,  3.,  3.,  3.],\n",
            "         [ 4.,  5.,  6.,  7.],\n",
            "         [ 1.,  2.,  3.,  4.]]])\n",
            "\n",
            "After LayerNorm:\n",
            " tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
            "         [-0.4472, -1.3416,  0.4472,  1.3416]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
            "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
            "         [-1.3416, -0.4472,  0.4472,  1.3416]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Example of use\n",
        "n_embd = 4\n",
        "sequence_length = 3\n",
        "\n",
        "x = torch.tensor([\n",
        "    [[1.0, 2.0, 3.0, 4.0],\n",
        "     [2.0, 2.0, 2.0, 2.0],\n",
        "     [0.0, -1.0, 1.0, 2.0]],\n",
        "    [[3.0, 3.0, 3.0, 3.0],\n",
        "     [4.0, 5.0, 6.0, 7.0],\n",
        "     [1.0, 2.0, 3.0, 4.0]]\n",
        "])\n",
        "\n",
        "ln = LayerNorm(n_embd)\n",
        "\n",
        "# Apply normalization\n",
        "out = ln(x)\n",
        "\n",
        "print(\"Input:\\n\", x)\n",
        "print(\"\\nAfter LayerNorm:\\n\", out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73nAYn9qdBmu"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, n_embd, head_size, sequence_length, dropout):\n",
        "    super().__init__()\n",
        "    # We are projecting (or linearly mapping) each token's embedding vector\n",
        "    # from the full embedding dimension (for instance n_embd = 256) down\n",
        "    # into the smaller subspace (for instance head_size = 8)\n",
        "    # head_size = n_embd/n_head\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    # torch.ones(sequence_length, sequence_length) creates a square matrix full of 1's\n",
        "    # torch.tril(...)\n",
        "    # keeps only the lower-triangular part of the matrix (including the diagonal)\n",
        "    # Example:\n",
        "    # [[1, 0, 0, 0],\n",
        "    # [1, 1, 0, 0],\n",
        "    # [1, 1, 1, 0],\n",
        "    # [1, 1, 1, 1]]\n",
        "\n",
        "    # self.register_buffer('tril', ...) stores the matrix as a non-trainable buffer inside the model.\n",
        "    # it becomes part of the model's state (so it's saved with state_dict())\n",
        "    # it is not a parameter (so it doesn't get gradients or updates)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(sequence_length, sequence_length)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # B = (Batch size) Number of sequences (samples) processed in parallel.\n",
        "    # T = (Sequence length (tokens per sample)) how many tokens we process at once (context window).\n",
        "    # C = Channel (embedding dimension) how many features represent each token.\n",
        "    B, T, C = x.shape\n",
        "    # k=x@WkT , x has shape (B, T, C), Wk​ has shape(head_size, n_embd)\n",
        "    k = self.key(x) # Projection of x, shape (B, T, head_size)\n",
        "    q = self.query(x) # Projection of x, shape (B, T, head_size)\n",
        "    # Compute the attention score\n",
        "    # k.transpose(-2, -1) swaps the last two dimensions (B, T, C_head) @ (B, C_head, T) = (B, T, T)\n",
        "    # k.size(-1) gives the dimension of the last axis, which is head_size\n",
        "    att = q @ k.transpose(-2, -1) * (1.0 / math.sqrt(k.size(-1)))\n",
        "    # self.tril is a lower triangular matrix like:\n",
        "    # [[1, 0, 0, 0],\n",
        "    # [1, 1, 0, 0],\n",
        "    # [1, 1, 1, 0],\n",
        "    # [1, 1, 1, 1]]\n",
        "    # self.tril[:T, :T] == 0 creates a boolean mask marking future positions (upper triangle) as True\n",
        "    # masked_fill(..., float('-inf')) replaces those future positions in the attention matrix with -inf\n",
        "    # [[  a, -inf, -inf],\n",
        "    # [  b,   c, -inf],\n",
        "    # [  d,   e,   f]]\n",
        "    att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    # Turns those raw scores into normalized probabilities across each row.\n",
        "    # How much attention a token shoul give to each previous token.\n",
        "    # All those -inf entries become zero probability (since exp(-inf) = 0).\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    att = self.dropout(att)\n",
        "    v = self.value(x) # Projection of x, shape (B, T, head_size)\n",
        "    # We use these probabilities to take the weighted average of all value vectors\n",
        "    # This gives each token a context vector, a blend of information from tokens it attends to most.\n",
        "    out = att @ v # (B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PvkU7Svd9U1",
        "outputId": "610694c8-9e33-4365-caea-04b79de6cff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:  torch.Size([2, 4, 16])\n",
            "Output shape:  torch.Size([2, 4, 8])\n",
            "Output tensor:\n",
            " tensor([[[ 1.6061, -0.0086, -0.0920,  0.4729, -0.4567, -0.3594,  0.5369,\n",
            "          -0.3031],\n",
            "         [-0.4740, -0.1296,  0.2600,  0.1935,  0.5968,  0.4383, -0.3777,\n",
            "           0.9747],\n",
            "         [ 0.8660, -0.3656, -0.0132, -0.1348, -0.2070, -0.0839,  0.2118,\n",
            "          -0.1921],\n",
            "         [ 0.3591, -0.3880,  0.1927, -0.1519, -0.1711, -0.0327,  0.1570,\n",
            "          -0.1644]],\n",
            "\n",
            "        [[ 0.1638,  0.3166, -1.1672, -1.5984,  0.0323,  0.3819, -0.7390,\n",
            "           0.7006],\n",
            "         [-0.0830,  0.6119, -0.2642, -0.8969,  0.2198,  0.2931, -0.4022,\n",
            "           1.1160],\n",
            "         [ 0.0759,  0.7122, -0.4415, -0.5050,  0.0148,  0.2933, -0.4520,\n",
            "           0.7690],\n",
            "         [-0.1249,  0.5125,  0.2879, -0.2315,  0.3353,  0.3216, -0.1415,\n",
            "           0.6094]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Example of use\n",
        "n_embd = 16\n",
        "head_size = 8\n",
        "sequence_length = 4\n",
        "dropout = 0.0\n",
        "\n",
        "# Create the attention head\n",
        "head = Head(n_embd, head_size, sequence_length, dropout)\n",
        "\n",
        "# Dummy input: batch of 2 sequences of 4 tokens, each with 16-dim embeddings\n",
        "x = torch.randn(2, sequence_length, n_embd)\n",
        "\n",
        "# Forward pass\n",
        "out = head(x)\n",
        "\n",
        "print(\"Input shape: \", x.shape)\n",
        "print(\"Output shape: \", out.shape)\n",
        "print(\"Output tensor:\\n\", out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrK0C3lvlmZs"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, n_embd, n_head, sequence_length, dropout):\n",
        "    super().__init__()\n",
        "    # Splits the embedding vector into n_head equal parts so each attention head\n",
        "    # works on one slice\n",
        "    head_size = n_embd // n_head\n",
        "\n",
        "    # List of n_head self-attention heads so the model can learn multiple types of\n",
        "    # relationships in parallel\n",
        "    # nn.ModuleList is like a Python list, but for PyTorch layers. It tells PyTorch\n",
        "    # that these are submodules and include their parameters in training.\n",
        "    self.heads = nn.ModuleList([\n",
        "        Head(n_embd, head_size, sequence_length, dropout) for _ in range(n_head)\n",
        "    ])\n",
        "    # Linear projection after concatenating all the heads.\n",
        "    # It recombines their outputs and lets the model learn\n",
        "    # how to best fuse the information across heads.\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # h(x) for h in self.heads: Runs the forward pass of each attention head individually.\n",
        "    # Each head returns an output tensor of shape (B, T, H)\n",
        "    # where:\n",
        "    # B = batch size\n",
        "    # T = sequence length\n",
        "    # H = head size (i.e., embedding size per head)\n",
        "    # torch.cat(..., dim=-1): Concatenates the results of all heads along the last dimension\n",
        "    # (the embedding dimension).\n",
        "    # If we have n_head heads, each of size H, the concatenated tensor has shape (B, T, n_head x H)\n",
        "    # this restores the same total embedding size n_embd\n",
        "    out = torch.cat([h(x) for h in self.heads], dim =-1)\n",
        "    # It recombines their outputs and lets the model learn\n",
        "    out = self.proj(out)\n",
        "    out = self.dropout(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT4LnDhVk9sZ",
        "outputId": "77875e52-73f6-4041-c1b3-ce7a9aa96a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 16, 64])\n",
            "Output shape: torch.Size([2, 16, 64])\n"
          ]
        }
      ],
      "source": [
        "# Example of use\n",
        "n_embd = 64 # Embedding dimension\n",
        "n_head = 8 # Number of attention heads\n",
        "sequence_length = 16 # Context window\n",
        "dropout = 0.1\n",
        "\n",
        "# Create a random batch of embeddings (e.g., batch of 2 sequences of 16 tokens each)\n",
        "x = torch.randn(2, sequence_length, n_embd)\n",
        "\n",
        "mha = MultiHeadAttention(n_embd, n_head, sequence_length, dropout)\n",
        "# Run forward pass\n",
        "out = mha(x)\n",
        "\n",
        "# Both should match\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Output shape:\", out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2AQnnTjnleR"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd, dropout):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        # Expands the embedding dimension (n_embd) into a larger hidden layer.\n",
        "        # This gives the model more capacity to transform information non-linearly.\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        # Activation function\n",
        "        nn.GELU(),\n",
        "        # Projects the data back down to the original embedding dimension.\n",
        "        # This way, the output shape matches what comes from the attention sub-layer, enabling\n",
        "        # the residual connection.\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        # Randomly zeros out some outputs during training (with probability dropout) to prevent overfitting.\n",
        "        # During inference, dropout is aoutomatically disabled.\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SaUhIhGuy_6",
        "outputId": "622eac0b-a046-4f18-fb27-cf918450a4f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:  torch.Size([2, 5, 64])\n",
            "Output shape:  torch.Size([2, 5, 64])\n"
          ]
        }
      ],
      "source": [
        "# Example of use\n",
        "n_embd = 64\n",
        "sequence_length = 5\n",
        "dropout = 0.1\n",
        "\n",
        "ff = FeedForward(n_embd, dropout)\n",
        "\n",
        "x = torch.randn(2, sequence_length, n_embd)\n",
        "\n",
        "# Forward pass\n",
        "out = ff(x)\n",
        "\n",
        "# Both should match\n",
        "print(\"Input shape: \", x.shape)\n",
        "print(\"Output shape: \", out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4XakEwPnJR5"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head, sequence_length, dropout):\n",
        "    super().__init__()\n",
        "    # Defines  a normalization layer that ensures each token's vector has stable\n",
        "    # mean and variance before it's processed by the self-attention mechanism.\n",
        "    self.ln1 = LayerNorm(n_embd)\n",
        "    self.sa = MultiHeadAttention(n_embd, n_head, sequence_length, dropout)\n",
        "    self.ln2 = LayerNorm(n_embd)\n",
        "    self.ffwd = FeedForward(n_embd, dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Take the input x, normalize it, pass it through the self-attention layer to get\n",
        "    # a refined representation, and then add that refinement back to the original x\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    # Normalize the current layer's ouput, feed it through a small neural network, and then\n",
        "    # add it back to the original preserving the core information while allowing refinement.\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cZjM6MExp-d",
        "outputId": "c9445638-b8c9-4585-af7d-ae05a2f2c0a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:  torch.Size([2, 16, 64])\n",
            "Output shape:  torch.Size([2, 16, 64])\n"
          ]
        }
      ],
      "source": [
        "# Example of use\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "sequence_length = 16\n",
        "dropout = 0.1\n",
        "\n",
        "block = Block(n_embd, n_head, sequence_length, dropout)\n",
        "x = torch.randn(2, sequence_length, n_embd)\n",
        "\n",
        "# Forward pass\n",
        "out = block(x)\n",
        "\n",
        "# Both should match\n",
        "print(\"Input shape: \", x.shape)\n",
        "print(\"Output shape: \", out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DERRPCCRMwEO"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "class GPTLanguageModel(nn.Module):\n",
        "  def __init__(self, config: GPTConfig):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(config.sequence_length, config.n_embd)\n",
        "    # Creates n_layer Transformer blocks (each with Multi-Head Self-Attention, Feed-Forward Network\n",
        "    # Residual connection and Layer Normalization)\n",
        "    self.blocks = nn.Sequential(*[\n",
        "        Block(config.n_embd, config.n_head, config.sequence_length, config.dropout)\n",
        "        for _ in range(config.n_layer)\n",
        "    ])\n",
        "    # self.blocks(x) runs the input through all Transformer blocks sequentially.\n",
        "    # Normalizes the final hidden representation from the last transformer block.\n",
        "    # Normalizes across the last dimension (the features of each token vector).\n",
        "    self.ln_f = LayerNorm(config.n_embd)\n",
        "    # lm = Language Modeling, head = Output head (final layer)\n",
        "    # Final projection layer that turns the model's internal hidden states into\n",
        "    # predictions over the vocabulary.\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "    # Inside the GPT model we have two related embedding layers\n",
        "    # self.token_embedding_table = nn.Embedding(vocab_size, n_embd) , self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "    # By default these two layers are separate, meaning:\n",
        "    # One set of weights (token_embedding_table.weight) is used to look up token embeddings.\n",
        "    # Another (lm_head.weight) is used to project back to the vocabulary space.\n",
        "    # Use the same matrix for both econding tokens into vectors and decoding vectors back into token logits\n",
        "    self.lm_head.weight = self.token_embedding_table.weight\n",
        "\n",
        "  # idx is a tensor of integers, where each integer corresponds to a character (or token)\n",
        "  # The model can be used in two modes:\n",
        "  # Training mode: We pass both idx(inputs) and targets(expected next tokens).\n",
        "  # Then it computes a loss for optimization.\n",
        "  # Inference mode: We only pass idx. The model return logits (predictions) but no loss\n",
        "  def forward(self, idx, targets: Optional[torch.Tensor] = None):\n",
        "    # B = batch size, T = Sequence length of the current input sequence\n",
        "    B, T = idx.shape\n",
        "    assert T <= self.config.sequence_length, \"Sequence length exceeds model limit\"\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    # Creates a 1D tensor of integers from 0 to T-1\n",
        "    # Example if T = 5\n",
        "    # tensor([0, 1, 2, 3, 4])\n",
        "    # .unsqueeze(0) adds a new dimension at index 0, turning the shape from\n",
        "    # (T,) into (1, T)\n",
        "    pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
        "    pos_emb = self.position_embedding_table(pos)\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      # V = vocabulary size\n",
        "      B,T,V = logits.shape\n",
        "      # Compare the model's predicted probabilities (logits) for each token position in every batch\n",
        "      # against the correct next-token(targets), and compute how wrong the model is - averaged over\n",
        "      # all tokens\n",
        "      loss = F.cross_entropy(logits.view(B*T, V), targets.view(B*T))\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  # idx: Input context, the seed phrase\n",
        "  # max_new_tokens: How many new tokens to generate after the initial context.\n",
        "  # temperature: Controls randomness of generation by scaling the logits.\n",
        "  # top_k: Optional[int]: This limits sampling to the top-k most probable tokens at each step.\n",
        "  @torch.no_grad()\n",
        "  def generate(self, idx: torch.Tensor, max_new_tokens: int, temperature: float = 1.0, top_k: Optional[int] = None):\n",
        "    # Switches the model into evaluation mode.\n",
        "    # It tells all submodules (like Dropout, BatchNorm, etc) to behave differently than during training.\n",
        "    self.eval()\n",
        "    for _ in range(max_new_tokens):\n",
        "      # This slices the tensor so that, if the generated sequence gets longer than the model's sequence length (context windows)\n",
        "      # we only keep the most recent sequence_length tokens.\n",
        "      idx_cond = idx[:, -self.config.sequence_length:]\n",
        "      logits, _ = self(idx_cond)\n",
        "      # Take the model's prediction for the last time step (the last token in the current sequence),\n",
        "      # after this the shape becomes (batch_size, vocab_size). Finally apply temperature scaling.\n",
        "      logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
        "      if top_k is not None:\n",
        "        # Top k highest values in each row of logits\n",
        "        v, _ = torch.topk(logits, top_k)\n",
        "        # Selects the last column of the top-k logits what implies take the smallest value amoing\n",
        "        # the top-k logits in each row.\n",
        "        thresh = v[:, [-1]]\n",
        "        # It zeros out the probability of all tokens not in the top-k, ensuring only the top-k\n",
        "        # tokens can be sampled next.\n",
        "        logits = torch.where(logits < thresh, torch.full_like(logits, -1e10), logits)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # Randomly samples one index from the probability distribution in probs.\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        # The two tensors are concatenated along dimension 1, which corresponds to the sequence length axis.\n",
        "        idx = torch.cat((idx, next_id), dim=1)\n",
        "    return idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnlms9PiuQRB"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK22Guqkv0LT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ViyxLikwKOD",
        "outputId": "ecde018a-5bfa-402f-8fad-ad57c0ebb4b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1F8sLvpbOYg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Data path\n",
        "file_path = \"/content/drive/MyDrive/ML/fine-tuning-gpt2/dostoevsky_clean.txt\"\n",
        "# Model path\n",
        "model_path = \"/content/drive/MyDrive/ML/transformer_architecture/mini_gpt_model.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZUuNbB4vIKK",
        "outputId": "be29979a-18c4-4f43-e50c-bc3d57d7da6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded. Total characters: 5651943\n"
          ]
        }
      ],
      "source": [
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(f\"Dataset loaded. Total characters: {len(text)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_svGbrLTvxYC"
      },
      "outputs": [],
      "source": [
        "text = text[:500_000]  # 500k characters, ~2.6 hours training time, Much faster experimentation\n",
        "tokenizer = CharTokenizer(text)\n",
        "# Encode the entire corpus\n",
        "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
        "# Split: 80% train, 10% validation, 10% test\n",
        "n = len(data)\n",
        "train_ratio = 0.8\n",
        "val_ratio = 0.1\n",
        "\n",
        "train_end = int(train_ratio * n)\n",
        "val_end = int((train_ratio + val_ratio) * n)\n",
        "\n",
        "train_data = data[:train_end]\n",
        "val_data = data[train_end:val_end]\n",
        "test_data = data[val_end:]\n",
        "\n",
        "sequence_length = 256\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T82SXx7NxA4y"
      },
      "outputs": [],
      "source": [
        "train_dataset = TextDataset(train_data, sequence_length)\n",
        "val_dataset = TextDataset(val_data, sequence_length)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# Consider not shuffle to val dataset\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize model\n",
        "config = GPTConfig(vocab_size=tokenizer.vocab_size)\n",
        "model = GPTLanguageModel(config).to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ypwsak9VdU7",
        "outputId": "bc82a1fc-eb31-4799-8a75-637afe84c31d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqCJ0JEu49N9",
        "outputId": "58a5c300-58ae-4dcd-9f4c-92a1b06eb95f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading trained model...\n",
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Choose mode: \"train\" or \"generate\"\n",
        "mode = \"generate\"\n",
        "\n",
        "if mode == \"train\":\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    step = 0\n",
        "    num_epochs = 3  # how many times we go through the entire dataset\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            _, loss = model(xb, yb)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Increment step count\n",
        "            step += 1\n",
        "\n",
        "            # Print every 100 batches\n",
        "            if step % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs} | Step {step:5d} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved at {model_path}\")\n",
        "\n",
        "else:\n",
        "    print(\"Loading trained model...\")\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2Etwkd27c8S",
        "outputId": "b26ae25f-8640-4e25-d912-832c05865388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Generated Text Sample ===\n",
            "\n",
            "The meaning of life is in the street and disappeared.\n",
            "\n",
            "“You have been besome to stupid into every one?”\n",
            "\n",
            "“I am? Well, that then you answer you do not to care the landlord!”\n",
            "\n",
            "“And what mistress? Madame liftes shall out lean\n"
          ]
        }
      ],
      "source": [
        "# Text generation\n",
        "prompt = \"The meaning of life is\"\n",
        "# Converts the list of token IDs into PyTorch tensor\n",
        "context = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long, device=device)\n",
        "\n",
        "generated = model.generate(\n",
        "    context,\n",
        "    max_new_tokens=200,\n",
        "    temperature = 0.9,\n",
        "    top_k = 50\n",
        ")\n",
        "\n",
        "# It converts model predictions (numbers) to text (characters)\n",
        "output_text = tokenizer.decode(generated[0].tolist())\n",
        "print(\"\\n=== Generated Text Sample ===\\n\")\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LHUGAhw3pg4w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPYXAJQ2KD9ekfGTkCZOQmd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}